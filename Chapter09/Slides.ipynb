{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed45c11-4da2-4e45-92a4-864b5a7095d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Chapter 9: Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "\n",
    "Annonomous ML researcher: \n",
    "\n",
    "\"Haha ... no one really does that (SVM) any more\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70dbee9d-c1eb-4ec8-9f5c-fa4a2665295b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Geometry\n",
    "---\n",
    "\n",
    "This technique is very Geometric - having some intuition for the concepts might help later. This is just a quick review.\n",
    "\n",
    "### Map stuff\n",
    "---\n",
    "\n",
    "We are going to use $\\mathbb{R}^n$ as an $\\mathbb{R}$ vector space exclusively but a lot of this is more general.\n",
    "\n",
    "> **Dot product**\n",
    "> \n",
    "> For two vectors $\\overline{x} = (x_1, x_2, \\ldots, x_n), \\overline{y} = (y_1, y_2, \\ldots, y_n) \\in \\mathbb{R}^n$ we define the dot product to be:\n",
    "> $$\\overline{x} \\cdot \\overline{y} = \\sum_{i=1}^n x_i y_i$$\n",
    "> Therefore the dot product is a map $- \\cdot - : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Intuitively think of the dot product $\\overline{x} \\cdot \\overline{y}$ as $\\overline{x}$'s component in the $\\overline{y}$'s direction (similarly vice versa).\n",
    "\n",
    "> **Basis example**\n",
    "> \n",
    "> Let $n = 2$ and suppose $\\overline{x} = (1,0)$ then $\\overline{x} \\cdot - : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ is that map that takes $(y_1, y_2) \\mapsto y_1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb795f1-23f7-49de-b471-e27230cb3a4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual example\n",
    "\n",
    "![Dot_product.png](./image/Dot_product.png)\n",
    "\n",
    "(Hilariously misleading - but it has all the feels!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fa4fe-0806-4328-883c-217b3f776945",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalisation\n",
    "\n",
    "This idea generalises to an inner product.\n",
    "\n",
    "> **Inner product**\n",
    "> \n",
    "> This is a map $\\langle - , - \\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that follows some rules:\n",
    "> 1. **Symetry**: $\\langle x, y \\rangle = \\langle y, x \\rangle$\n",
    "> 2. **Linearity**: $\\langle ax_1 + bx_2, y \\rangle = a \\langle x_1, y \\rangle + b \\langle x_2, y \\rangle$\n",
    "> 3. **Positive-definiteness**: $\\langle x, x \\rangle > 0$\n",
    "\n",
    "You do not need to remember this - just that the idea generalises (for later).\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Dot product**\n",
    "> $$\\overline{x} \\cdot \\overline{y} = \\sum_{i=1}^n x_i y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450e7f5-2e06-40e0-b390-c343f6bb0494",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Hyperplanes\n",
    "---\n",
    "\n",
    "You should think of a Hyperplan as a straight cut of the space into two segments. This is done in $\\mathbb{R}^n$ by a $\\mathbb{R}^{n-1}$ subplane (not space!).\n",
    "\n",
    "> **Hyperplane**\n",
    ">\n",
    "> A hyperplane $H$ in $\\mathbb{R}^n$ is given by a linear equaltion $\\sum_{i=1}^n t_i x_i = c$ for $t_i, c \\in \\mathbb{R}$. This is\n",
    "> $$H = \\{ (x_1, x_2, \\ldots, x_n) \\in \\mathbb{R}^n \\vert \\sum_{i=1}^n t_i x_i = c \\}.$$\n",
    "\n",
    "Whilst this definition is correct it is sometimes helpful to think about this in terms of the dot product earlier. Choose a vector $t = (t_1, t_2, \\ldots, t_n)$ and some constant $c \\in \\mathbb{R}$ then\n",
    "$$ H_{t,c} = \\{ \\overline{x} \\in \\mathbb{R}^n \\vert t \\cdot \\overline{x} = c\\}.$$\n",
    "Geometrically $t$ is a perpendicular vector to our plane and $c$ is the distance along that perpendicular vector the plane intersects.\n",
    "\n",
    "This intuition naturally divides out plane two $H_- = \\{\\overline{x} \\in \\mathbb{R}^n \\vert t \\cdot \\overline{x} < c\\}$ and $H_+ = \\{\\overline{x} \\in \\mathbb{R}^n \\vert t \\cdot \\overline{x} > c\\}$.\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Dot product**\n",
    "> $$\\overline{x} \\cdot \\overline{y} = \\sum_{i=1}^n x_i y_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "291a4577-3ea0-4a59-a0af-bf048bebbb78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual example\n",
    "\n",
    "![Hyperplane.png](./image/Hyperplane.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95912b3c-06a0-4ce2-bd44-e0db216cc428",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Uniqueness of expression\n",
    "\n",
    "Due to the linearity of the dot product $H_{t,c} = H_{st, c/s}$ for any $s \\in \\mathbb{R}_{>0}$.\n",
    "\n",
    "So when expressing hyperplanes it simplifies things to just talk about $t \\in \\mathbb{R}^n$ such that $t \\cdot t = 1$, these are the unit vectors.\n",
    "\n",
    "What about $s = -1$? Whilst technically this defines the same hyperplane it flips the negative and positive cuts - so we consider $H_{t,c} \\not = H_{-t,-c}$.\n",
    "\n",
    "----\n",
    "Recap:\n",
    "\n",
    "> **Hyperplanes**\n",
    "> \n",
    "> Hyperplanes are uniquely determined by a unit vector $t \\in \\mathbb{R}^n$ ($t \\cdot t = 1$) and a constant $c \\in \\mathbb{R}$:\n",
    "> $$H_{t,c} = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t = c\\}$$\n",
    "> This cuts the plane into a positive $H_+ = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t > c$ and negative region $H_- = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t < c\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11042f46-3603-432a-9b36-fb49b2c15064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The long road ahead\n",
    "\n",
    "Support vectors are used for classification problems. Therefore we catagories $C$, and training data that associated a point in $\\mathbb{R}^n$ to catagory in $C$ i.e. $D \\subset \\mathbb{R}^n \\times C$. \n",
    "\n",
    "We will get to Support Vector Machines in 3 hops! (These will always assume $\\vert C \\vert = 2$.)\n",
    "\n",
    "- Maximal margin classifier: Your data can be seperated by a Hyperplane.\n",
    "- Support vector **classifier**: Your data has a \"linear decision boundary\".\n",
    "- Support vector machine: You data has a non-linear decision boundary.\n",
    "\n",
    "We will also talk about the more than 2 catagory case (if we have time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8cd528-d290-4610-a22c-fc08009ae750",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Maximal margin classifier\n",
    "\n",
    "---\n",
    "\n",
    "> **Linearly seperable**\n",
    ">\n",
    "> Two sets of point $X_1, X_2 \\subset \\mathbb{R}^n$ are *linearly seperable* if there is some hyperplane $H_{t,c}$ that seperates them. I.e. $X_1 \\subset H_-$ and $X_2 \\subset H_+$.\n",
    "\n",
    "In fact if some hyperplan seperates our data, infinitely many hyperplanes do. \n",
    "\n",
    "Let $l = \\max\\{x_1 \\cdot t \\vert x_1 \\in X_1\\}$ and $h = \\min\\{x_2 \\cdot t \\vert x_2 \\in X_2\\}$ then for each $c \\in (l,h)$ $H_{t,c}$ seperates them. \n",
    "\n",
    "Though there might be other choices of $t$ as well!\n",
    "\n",
    "So which one is best?\n",
    "\n",
    "----\n",
    "Reminder:\n",
    "\n",
    "> **Hyperplanes**\n",
    "> \n",
    "> Hyperplanes are uniquely determined by a unit vector $t \\in \\mathbb{R}^n$ ($t \\cdot t = 1$) and a constant $c \\in \\mathbb{R}$:\n",
    "> $$H_{t,c} = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t = c\\}$$\n",
    "> This cuts the plane into a positive $H_+ = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t > c$ and negative region $H_- = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t < c\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cbdcf3c-a382-44b6-b59f-19a98037c5e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual example\n",
    "\n",
    "![linearly_seperable.png](./image/linearly_seperable.png)\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Linearly seperable**\n",
    ">\n",
    "> Two sets of point $X_1, X_2 \\subset \\mathbb{R}^n$ are *linearly seperable* if there is some hyperplane $H_{t,c}$ that seperates them. I.e. $X_1 \\subset H_-$ and $X_2 \\subset H_+$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fde2f-a47a-459f-a985-cd82585b9e10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Margin\n",
    "---\n",
    "\n",
    "Notationally it is easiest not if we assume our catagories are $C = \\{-1, 1\\}$. We say $y_1 = -1$ and $y_2 = 1$ which are the labels for $X_1$ and $X_2$ respectively.\n",
    "\n",
    "The margin of a hyperplane $H_{t,c}$ and the linearly seperable data $X_1$, and $X_2$. The distance of the closest point in $X_1$ and $X_2$ to the line $H_{t,c}$.\n",
    "\n",
    "> **Margin**\n",
    ">\n",
    "> For some linearly seperable data $X_1$, and $X_2$ the *margin* of a Hyperplane $H_{t,a}$ that seperates them is:\n",
    "> $$M = \\min\\{ (t \\cdot x_i - c) y_i \\vert x_i \\in X_i\\}.$$\n",
    "\n",
    "Intuitively we want to maximise this to make the least assumptions about the structure of the real data from the training data. \n",
    "\n",
    "----\n",
    "Reminder:\n",
    "\n",
    "> **Hyperplanes**\n",
    "> \n",
    "> Hyperplanes are uniquely determined by a unit vector $t \\in \\mathbb{R}^n$ ($t \\cdot t = 1$) and a constant $c \\in \\mathbb{R}$:\n",
    "> $$H_{t,c} = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t = c\\}$$\n",
    "> This cuts the plane into a positive $H_+ = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t > c$ and negative region $H_- = \\{\\overline{x} \\in \\mathbb{R}^n \\vert \\overline{x} \\cdot t < c\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f5ffc8a-88fe-40f4-a852-a26fe75839e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual example\n",
    "\n",
    "![margin.png](./image/margin.png)\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Margin**\n",
    ">\n",
    "> For some linearly seperable data $X_1$, and $X_2$ the *margin* of a Hyperplane $H_{t,a}$ that seperates them is:\n",
    "> $$M = \\min\\{ (t \\cdot x_i - c) y_i \\vert x_i \\in X_i\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53491400-e893-4d69-9309-919699d74653",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Classifier definition\n",
    "\n",
    "> **Maximal marginal classifier**\n",
    ">\n",
    "> For linearly seperable data $X_1$ and $X_2$ the hyperplane $H_{t,a}$ is the *maximal margin classifier* if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M & \\mbox{ for all } x_i \\in X_i \\end{align*}$$\n",
    "\n",
    "There are two nice properties that come from the maximal marginal classifier:\n",
    "1. The closes point is $X_1$ to the hyperplane is the same distance as $X_2$.\n",
    "2. It is unique!\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "We assume the catagories for $X_1$ is $y_1 = -1$ and $X_2$ is $y_2 = 1$.\n",
    "\n",
    "> **Margin**\n",
    ">\n",
    "> For some linearly seperable data $X_1$, and $X_2$ the *margin* of a Hyperplane $H_{t,a}$ that seperates them is:\n",
    "> $$M = \\min\\{ (t \\cdot x_i - c) y_i \\vert x_i \\in X_i\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea1e12-d276-481c-a84c-b96b2ad03e62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Visual example\n",
    "\n",
    "![mmc](./image/maximal_marginal_classifier.png)\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Maximal marginal classifier**\n",
    ">\n",
    "> For linearly seperable data $X_1$ and $X_2$ the hyperplane $H_{t,a}$ is the *maximal margin classifier* if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M & \\mbox{ for all } x_i \\in X_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13399e16-1f60-4b4b-b2b3-597e5238e9f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Support vectors\n",
    "\n",
    "We call the closest points to the maximal marginal classifer *support vectors*.\n",
    "\n",
    ">**Support Vector**\n",
    ">\n",
    ">For linearly seperable data $X_1$ and $X_2$ a maximal marginal classifier $H_{t,a}$ of margin $M$ has support vectors $S_1 \\subset X_1$ and $S_2 \\subset X_2$ such that\n",
    ">$$((s_i \\cdot t) - c)y_i = M  \\ \\mbox{ for } s_i \\in S_i.$$\n",
    ">Both $S_1$ and $S_2$ are non-empty.\n",
    "\n",
    "The classifier is only determined by the support vectors - the other data points can be moved as long as their margin is larger the $M$.\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Maximal marginal classifier**\n",
    ">\n",
    "> For linearly seperable data $X_1$ and $X_2$ the hyperplane $H_{t,a}$ is the *maximal margin classifier* if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M & \\mbox{ for all } x_i \\in X_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860c386-4154-483f-af1a-38bde7d4e241",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quick aside (Neural networks)\n",
    "\n",
    "The most basic unit of a neural network is a *Perceptron*. In the most basic case this is a classifier.\n",
    "\n",
    ">**Perceptron**\n",
    ">\n",
    ">A *perceptron* is a function $p: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ defined by two steps - first a weighted linear sum, the second an *activation function*. To define this we need weights $w_i$ for $1 \\leq i \\leq n$ and an activation function $a: \\mathbb{R} \\rightarrow \\mathbb{R}$. Where\n",
    ">$$p(x) = a\\left (\\sum_{i=1}^n w_i x_i \\right ).$$\n",
    "\n",
    ">**Binary step (activation function)**\n",
    ">\n",
    ">*Binary step* is an *cctivation function* that determines what the value is in reference to a parameter $\\theta$.\n",
    ">$$a_{\\theta}(x) = \\begin{cases} 1 & \\mbox{if } x \\geq \\theta\\\\ -1 & \\mbox{if } x < \\theta . \\end{cases}$$\n",
    "\n",
    "When we train a perceptron using the \"perceptron rule\" on linearly seperable data using the binary step function - this is really training a marginal classifier with perpendicular vector $w = (w_1, w_2, \\ldots, w_n)$ and constant $\\theta$.\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Dot product**\n",
    "> $$\\overline{x} \\cdot \\overline{y} = \\sum_{i=1}^n x_i y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f2722-587e-4b4c-8382-f8c947114a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Support vector classifier\n",
    "---\n",
    "\n",
    "So in reality our data is nearly never linearly seperable.\n",
    "\n",
    "Though maybe we can be a little less strict on the margin boundary?\n",
    "\n",
    "Suppose for each point of data $(x_i, y_i)$ we allow for a small error $\\epsilon_i \\geq 0$. The error can potenially violate the margin in the following way\n",
    "\n",
    "$$\n",
    "((x_i \\cdot t) - c)y_i \\geq M(1- \\epsilon_i).\n",
    "$$\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Maximal marginal classifier**\n",
    ">\n",
    "> For linearly seperable data $X_1$ and $X_2$ the hyperplane $H_{t,a}$ is the *maximal margin classifier* if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M & \\mbox{ for all } x_i \\in X_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6569376-5399-4c65-b859-ae3329b9b4a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual demo (error)\n",
    "\n",
    "![vector_support_classifier](./image/vector_support_classifier.png)\n",
    "\n",
    "---\n",
    "\n",
    "Reminder: We are making the following relaxation\n",
    "$$\n",
    "((x_i \\cdot t) - c)y_i \\geq M(1- \\epsilon_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ac5b4-171b-4c5e-ad15-30f6adca48ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Classifier definition\n",
    "\n",
    "> **Support vector classifier**\n",
    ">\n",
    "> For classification data $X = \\{(x_i,y_i) \\vert x_i \\in \\mathbb{R}^n, y_i \\in \\{1,-1\\}, i \\in I\\}$ the hyperplane $H_{t,a}$ is the *support vector classifier* with error $E \\geq 0$ if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, \\epsilon_i, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M(1-\\epsilon_i) & \\mbox{ for all } i \\in I\\\\ & \\epsilon_i \\geq 0& \\mbox{ for all } i \\in I\\\\ & \\sum_{i \\in I} \\epsilon_i \\leq E. \\end{align*}$$\n",
    "\n",
    "When choosing an error you have natural pay off - higher error in classification but a larger margin.\n",
    "\n",
    "Note: You may want to add an error term even if your data is linearly seperable if you have support vectors very close to the decision boundy and want to decrease the variance.\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Maximal marginal classifier**\n",
    ">\n",
    "> For linearly seperable data $X_1$ and $X_2$ the hyperplane $H_{t,a}$ is the *maximal margin classifier* if it solves the following optimisation problem:\n",
    "> $$\\begin{align*} & \\max_{t, c, M} M\\\\ & t \\cdot t = 1 \\\\ & ((x_i \\cdot t) - c)y_i \\geq M & \\mbox{ for all } x_i \\in X_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afe33f-e57e-46b7-b330-4f9f032656bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How do we actually solve this?\n",
    "\n",
    "Essentially magic. *This is not important only used for intuition for the next section!*\n",
    "\n",
    "This problem is a Quadratic programming problem (like a linear programming problem but with powers of 2!). Instead of solving the problem directly we can investigate the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "& \\max_{\\alpha} \\sum_{i \\in I} \\alpha_i - \\frac{1}{2} \\sum_{i,j \\in I} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\\\\\n",
    "& E \\geq \\alpha_i \\geq 0 & \\mbox{ for all } i \\in I\\\\\n",
    "& \\sum_{i \\in I} \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We get the original parameters by setting: \n",
    "$$\n",
    "\\begin{align*}\n",
    "& c = - y_j + \\sum_{i \\in I} \\alpha_i y_i (x_i \\cdot x_j) & \\mbox{ for any } j \\in I \\mbox{ such that } \\alpha_j \\not = 0\\\\\n",
    "& t = \\sum_{i \\in I} \\alpha_i y_i x_i\n",
    "\\end{align*}\n",
    "$$\n",
    "Though we will not use this - instead we will classsifty points by\n",
    "$$\n",
    "\\hat{f}(x) = \\mbox{sgn}\\left ( \\sum_{i \\in I} \\alpha_i y_i (x_i \\cdot x) - c \\right )\n",
    "$$\n",
    "\n",
    "**Important note**: In the optimisation problem we don't need $x_i$ by itself only $x_i \\cdot x_j$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ebe2f-96cf-45f4-9346-c3834b192392",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Most of the time our decision boundary is far from linear. For example consider exclusive or $\\oplus$.\n",
    "\n",
    "![xor.png](./image/xor.png)\n",
    "\n",
    "---\n",
    "Note: Exclusive or is a binary operation which is only true if only one complement is true i.e. $T \\oplus F = F \\oplus T = T$ and $T \\oplus T = F \\oplus F = F$.\n",
    "\n",
    "For the pciture above we let $x_1$ be the value of the first coordinate 1 for True and -1 for False and $x_2$ be the value of the second. This gives us the following 4 data points: $D = \\{ ((1,1), -1), ((1,-1), 1), ((-1,1), 1), ((-1, -1), -1) \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c805b-1479-4e3f-92fb-4e9168da06fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Going to higher dimensions\n",
    "\n",
    "If we look at xor in $\\mathbb{R}^2$ we can't use what is above however we can embed it in a larger space. Consider the following mapping:\n",
    "\n",
    "$$K: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6, \\ \\mbox{ by } \\ (x_1, x_2) \\mapsto (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1)$$\n",
    "\n",
    "Then using a particular selection of basis we get the following data points\n",
    "$$\n",
    "\\begin{align*}\n",
    "((1,1), -1) & \\mapsto ((1, 1, \\sqrt{2}, \\sqrt{2}, \\sqrt{2}, 1), -1)\\\\\n",
    "((1,-1), 1) & \\mapsto ((1, 1, -\\sqrt{2}, \\sqrt{2}, -\\sqrt{2}, 1), 1)\\\\\n",
    "((-1,1), 1) & \\mapsto ((1, 1, -\\sqrt{2}, -\\sqrt{2}, \\sqrt{2}, 1), 1)\\\\\n",
    "((-1,-1), -1) & \\mapsto ((1, 1, \\sqrt{2}, -\\sqrt{2}, -\\sqrt{2}, 1), -1)\n",
    "\\end{align*}\n",
    "$$\n",
    "This seperates the data nicely on the 3rd coordinate. So we now just need to take our hyper plane defined by that $(0,0,1,0,0,0)$.\n",
    "\n",
    "---\n",
    "Note: Exclusive or is a binary operation which is only true if only one complement is true i.e. $T \\oplus F = F \\oplus T = T$ and $T \\oplus T = F \\oplus F = F$.\n",
    "\n",
    "For the pciture above we let $x_1$ be the value of the first coordinate 1 for True and -1 for False and $x_2$ be the value of the second. This gives us the following 4 data points: $D = \\{ ((1,1), -1), ((1,-1), 1), ((-1,1), 1), ((-1, -1), -1) \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c0709-da33-4f11-8bd7-db1b6c815fd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visualising the result\n",
    "\n",
    "![xor_embeded](./image/xor_embeded.png)\n",
    "\n",
    "---\n",
    "Our embedding: $$K: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6, \\ \\mbox{ by } \\ (x_1, x_2) \\mapsto (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1)$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "((1,1), -1) & \\mapsto ((1, 1, \\sqrt{2}, \\sqrt{2}, \\sqrt{2}, 1), -1)\\\\\n",
    "((1,-1), 1) & \\mapsto ((1, 1, -\\sqrt{2}, \\sqrt{2}, -\\sqrt{2}, 1), 1)\\\\\n",
    "((-1,1), 1) & \\mapsto ((1, 1, -\\sqrt{2}, -\\sqrt{2}, \\sqrt{2}, 1), 1)\\\\\n",
    "((-1,-1), -1) & \\mapsto ((1, 1, \\sqrt{2}, -\\sqrt{2}, -\\sqrt{2}, 1), -1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efda3a1-2eed-4e11-af65-1868e08e54ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Another example\n",
    "\n",
    "![polynomial_embedding.png](./image/polynomial_embedding.png)\n",
    "\n",
    "---\n",
    "Taken from: https://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4322d6-5516-4e31-832e-1677a0110623",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Formalising this approach\n",
    "\n",
    "As your data gets bigger increasing the feature space dramatically like above is going to get incredibly expense. However recall:\n",
    "\n",
    "In support vector classification we don't need $x_i$ by itself only $x_i \\cdot x_j$.\n",
    "\n",
    "> **Kernel**\n",
    ">\n",
    "> For an embedding $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ the *kernel* of the embedding\n",
    "> $$K_{\\Phi} : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    "> is given by $K_{\\Phi}(x,y) = \\Phi(x) \\cdot \\Phi(y)$.\n",
    "\n",
    "To apply support vector classification we do not need the embedding - only the Kernel of the embedding. (Note the Kernel is just an inner product as defined before!)\n",
    "\n",
    "---\n",
    "Reminder: Support vector classifier is given by:\n",
    "$$\n",
    "\\begin{align*} \n",
    "& \\max_{\\alpha} \\sum_{i \\in I} \\alpha_i - \\frac{1}{2} \\sum_{i,j \\in I} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\\\\\n",
    "& E \\geq \\alpha_i \\geq 0 & \\mbox{ for all } i \\in I\\\\\n",
    "& \\sum_{i \\in I} \\alpha_i y_i = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "With classification given by:\n",
    "$$\n",
    "\\hat{f}(x) = \\mbox{sgn}\\left ( \\sum_{i \\in I} \\alpha_i y_i (x_i \\cdot x) - c \\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72fb2b-d8f1-4396-a015-044ea726fe7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Popular Kernels\n",
    "\n",
    "> **Polynomial kernel**\n",
    ">\n",
    ">$$K_{d,c}: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}, \\mbox{ by } K_{d,c}(x_1, x_2) = (x \\cdot y + c)^d$$\n",
    "\n",
    "> **Gaussian kernel**\n",
    ">\n",
    ">$$K_{\\sigma}: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}, \\mbox{ by } K_{\\sigma}(x_1, x_2) = \\exp \\left ( - \\frac{\\vert \\vert x - y \\vert \\vert^2}{2 \\sigma^2} \\right )$$\n",
    "\n",
    "> **Sigmoid kernel**\n",
    ">\n",
    ">$$K_{a,b}: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}, \\mbox{ by } K_{a,b}(x_1, x_2) = \\tanh(a \\cdot (x \\cdot y) + b)$$\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "> **Kernel**\n",
    ">\n",
    "> For an embedding $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ the *kernel* of the embedding\n",
    "> $$K_{\\Phi} : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    "> is given by $K_{\\Phi}(x,y) = \\Phi(x) \\cdot \\Phi(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf32d69-18bb-42b2-b0e3-ed9a6f25faf9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Custom Kernels\n",
    "\n",
    "The kernel is really just a measure of similarity between two points. So can I pick any Kernel (or similarity) function and the associated embedding exists?\n",
    "\n",
    "Quick answer: No!\n",
    "\n",
    "The function $K: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a kernal of some embedding if:\n",
    "1. It is symetric $K(x,y) = K(y,x)$.\n",
    "2. It is continuous.\n",
    "3. It must have a semi-positive definiate Gram-matrix .... yeah me neither.\n",
    "\n",
    "---\n",
    "Reminder:\n",
    "\n",
    "> **Kernel**\n",
    ">\n",
    "> For an embedding $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ the *kernel* of the embedding\n",
    "> $$K_{\\Phi} : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    "> is given by $K_{\\Phi}(x,y) = \\Phi(x) \\cdot \\Phi(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a86273-595e-4655-bf75-bfa9be480fe9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multiple classes\n",
    "\n",
    "To be honest the book suggests the kind of obvious things to do here. Suppose you have $k$ classes you can do one of the following:\n",
    "\n",
    "- Do pairwise comparisons of different classes generating $\\binom{k}{2}$ SVM. Then classify points based on which class they are most consistently classifed as.\n",
    "- Compare elements in that class to ones that are not, generating $k$ SVM. We then assign a test point to the class that has the highest confidence in its evaluation.\n",
    "\n",
    "---\n",
    "Reminder: Support vector classifier is given by:\n",
    "$$\n",
    "\\begin{align*} \n",
    "& \\max_{\\alpha} \\sum_{i \\in I} \\alpha_i - \\frac{1}{2} \\sum_{i,j \\in I} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\\\\\n",
    "& E \\geq \\alpha_i \\geq 0 & \\mbox{ for all } i \\in I\\\\\n",
    "& \\sum_{i \\in I} \\alpha_i y_i = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "With classification given by:\n",
    "$$\n",
    "\\hat{f}(x) = \\mbox{sgn}\\left ( \\sum_{i \\in I} \\alpha_i y_i (x_i \\cdot x) - c \\right ).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a6de6-beaf-4a71-a4cc-08b17946c1e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Why did my friend say no one uses it?\n",
    "\n",
    "SVM are powerful out the box methods that let you put your domain knowledge into the problem by picking the Kernal and error values. However their ingenuity lies in their ability to take advantage of costly high dimensional embeddings in a cheap way. \n",
    "\n",
    "With the dramatic reduction in cost of compute this competitive advantage is losing out to methods that allow for more customisation in the embedding stages.\n",
    "\n",
    "For example consider xor again. This was solved by a polynomial embedding into 6 dimensional space. We could do this or use a couple perceptrons like so:\n",
    "\n",
    " **DO this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ee5dc-14f3-4724-a6c0-73a15a8e340e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
